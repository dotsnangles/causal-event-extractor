{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-20 09:43:55.880567: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 09:43:58.922687: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "tokenizer = ElectraTokenizer.from_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['[PAD]', 'E_B', 'E_I', 'O']\n",
    "num_labels = len(labels)\n",
    "id2label = {k: v for k, v in enumerate(labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Preprocess for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda x: ['[CLS]'] + x + ['[SEP]'])\n",
    "data['labels'] = data.labels.apply(lambda x: ['O'] + x + ['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tokens.apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda x: x + ['[PAD]'] * (max_len - len(x)))\n",
    "data['labels'] = data.labels.apply(lambda x: x + ['[PAD]'] * (max_len - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lst = data.tokens.to_list()\n",
    "labels_lst = data.labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(tokens_lst, \n",
    "                                                    labels_lst, \n",
    "                                                    test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for tokens, labels in zip(X_train, y_train):\n",
    "    length = tokens.index('[PAD]')\n",
    "    mask = [1] * length + [0] * (max_len - length)\n",
    "    label_ids = []\n",
    "    for label in labels:\n",
    "        label_ids.append(label2id[label])\n",
    "        \n",
    "    train_data.append([tokenizer.convert_tokens_to_ids(tokens), mask, label_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "for tokens, labels in zip(X_eval, y_eval):\n",
    "    length = tokens.index('[PAD]')\n",
    "    mask = [1] * length + [0] * (max_len - length)\n",
    "    \n",
    "    label_ids = []\n",
    "    for label in labels:\n",
    "        label_ids.append(label2id[label])\n",
    "        \n",
    "    eval_data.append([tokenizer.convert_tokens_to_ids(tokens), mask, label_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = random.randrange(0, len(train_data) - 1)\n",
    "# for x, xm, y in zip(train_data[idx][0], train_data[idx][1], train_data[idx][2]):\n",
    "#     print(x, xm, y)\n",
    "\n",
    "# idx = random.randrange(0, len(eval_data) - 1)\n",
    "# for x, xm, y in zip(eval_data[idx][0], eval_data[idx][1], eval_data[idx][2]):\n",
    "#     print(x, xm, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "LEARNING_RATE = 5e-5\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and Generate Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggerDataset(Dataset): \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx][0]\n",
    "        mask = self.data[idx][1]\n",
    "        label_ids = self.data[idx][2]\n",
    "        return (torch.LongTensor(input_ids), torch.LongTensor(mask), torch.LongTensor(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggerDataset(train_data)\n",
    "eval_dataset = TaggerDataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPoSTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dim, \n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, mask):\n",
    "        embedded = self.dropout(self.bert(text, mask)[0])\n",
    "        predictions = self.fc(self.dropout(embedded))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(36223, 768)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIM = num_labels\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTPoSTagger(bert,\n",
    "                      OUTPUT_DIM, \n",
    "                      DROPOUT)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=0, factor=0.7, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NGPU = torch.cuda.device_count()\n",
    "if NGPU > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(NGPU)))\n",
    "    # model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
    "    # torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = -1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = torch.nonzero(y != tag_pad_idx)\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_f1(preds, y, tag_pad_idx):\n",
    "    max_preds = preds.argmax(dim = -1, keepdim = True) # get the index of the max probability\n",
    "    non_pad_elements = torch.nonzero(y != tag_pad_idx)\n",
    "    max_preds_no_pad = max_preds[non_pad_elements].squeeze(1).detach().cpu()\n",
    "    y_no_pad = y[non_pad_elements].detach().cpu()\n",
    "    \n",
    "    f1_macro = f1_score(y_no_pad, max_preds_no_pad, average='macro')\n",
    "    f1_micro = f1_score(y_no_pad, max_preds_no_pad, average='micro')    \n",
    "    \n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, tag_pad_idx):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    predictions_set = None\n",
    "    tags_set = None\n",
    "    for batch in iterator:\n",
    "        text = batch[0].to(device)\n",
    "        mask = batch[1].to(device)\n",
    "        tags = batch[2].to(device)\n",
    "\n",
    "        predictions = model(text, mask)\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        if predictions_set == None:\n",
    "            predictions_set = predictions\n",
    "            tags_set = tags\n",
    "        else:\n",
    "            predictions_set = torch.cat([predictions_set, predictions], dim=0)\n",
    "            tags_set = torch.cat([tags_set, tags], dim=0)\n",
    "        \n",
    "        loss = criterion(predictions, tags)\n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    f1_macro, f1_micro = categorical_f1(predictions_set, tags_set, tag_pad_idx)\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion, tag_pad_idx):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    predictions_set = None\n",
    "    tags_set = None\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text = batch[0].to(device)\n",
    "            mask = batch[1].to(device)\n",
    "            tags = batch[2].to(device)\n",
    "            \n",
    "            predictions = model(text, mask)\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "            \n",
    "            if predictions_set == None:\n",
    "                predictions_set = predictions\n",
    "                tags_set = tags\n",
    "            else:\n",
    "                predictions_set = torch.cat([predictions_set, predictions], dim=0)\n",
    "                tags_set = torch.cat([tags_set, tags], dim=0)\n",
    "            \n",
    "            loss = criterion(predictions, tags)\n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "\n",
    "        f1_macro, f1_micro = categorical_f1(predictions_set, tags_set, tag_pad_idx)\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 1m 5s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.810 | Train Acc: 71.30% | Train F1 Mac: 23.43% | Train F1 Mic: 71.47%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 77.64% |  Val. F1 Mac: 29.14% |  Val. F1 Mic: 77.65%\n",
      "\n",
      "Epoch: 02 | Epoch Time: 0m 17s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.639 | Train Acc: 77.07% | Train F1 Mac: 30.97% | Train F1 Mic: 76.81%\n",
      "\t Val. Loss: 0.524 |  Val. Acc: 80.43% |  Val. F1 Mac: 48.50% |  Val. F1 Mic: 80.49%\n",
      "\n",
      "Epoch: 03 | Epoch Time: 0m 17s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.519 | Train Acc: 79.96% | Train F1 Mac: 54.37% | Train F1 Mic: 79.87%\n",
      "\t Val. Loss: 0.494 |  Val. Acc: 81.70% |  Val. F1 Mac: 55.10% |  Val. F1 Mic: 81.66%\n",
      "\n",
      "Epoch: 04 | Epoch Time: 0m 17s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.429 | Train Acc: 83.70% | Train F1 Mac: 66.76% | Train F1 Mic: 83.47%\n",
      "\t Val. Loss: 0.333 |  Val. Acc: 87.32% |  Val. F1 Mac: 74.38% |  Val. F1 Mic: 87.35%\n",
      "\n",
      "Epoch: 05 | Epoch Time: 0m 17s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.324 | Train Acc: 87.74% | Train F1 Mac: 75.55% | Train F1 Mic: 87.62%\n",
      "\t Val. Loss: 0.277 |  Val. Acc: 89.91% |  Val. F1 Mac: 80.02% |  Val. F1 Mic: 89.85%\n",
      "\n",
      "Epoch: 06 | Epoch Time: 0m 18s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.274 | Train Acc: 90.12% | Train F1 Mac: 80.29% | Train F1 Mic: 90.05%\n",
      "\t Val. Loss: 0.252 |  Val. Acc: 91.10% |  Val. F1 Mac: 81.91% |  Val. F1 Mic: 91.10%\n",
      "\n",
      "Epoch: 07 | Epoch Time: 0m 17s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.243 | Train Acc: 91.39% | Train F1 Mac: 82.87% | Train F1 Mic: 91.35%\n",
      "\t Val. Loss: 0.236 |  Val. Acc: 91.47% |  Val. F1 Mac: 82.95% |  Val. F1 Mic: 91.50%\n",
      "\n",
      "Epoch: 08 | Epoch Time: 0m 18s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.212 | Train Acc: 92.52% | Train F1 Mac: 85.22% | Train F1 Mic: 92.70%\n",
      "\t Val. Loss: 0.223 |  Val. Acc: 92.06% |  Val. F1 Mac: 84.10% |  Val. F1 Mic: 92.05%\n",
      "\n",
      "Epoch: 09 | Epoch Time: 0m 17s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 0.192 | Train Acc: 93.34% | Train F1 Mac: 86.58% | Train F1 Mic: 93.34%\n",
      "\t Val. Loss: 0.240 |  Val. Acc: 91.77% |  Val. F1 Mac: 83.28% |  Val. F1 Mic: 91.78%\n",
      "\n",
      "Epoch: 10 | Epoch Time: 0m 18s\n",
      "Learning Rate: 3.5e-05\n",
      "\tTrain Loss: 0.162 | Train Acc: 94.45% | Train F1 Mac: 88.59% | Train F1 Mic: 94.42%\n",
      "\t Val. Loss: 0.214 |  Val. Acc: 92.75% |  Val. F1 Mac: 86.27% |  Val. F1 Mic: 92.77%\n",
      "\n",
      "Epoch: 11 | Epoch Time: 0m 17s\n",
      "Learning Rate: 3.5e-05\n",
      "\tTrain Loss: 0.132 | Train Acc: 95.49% | Train F1 Mac: 90.87% | Train F1 Mic: 95.50%\n",
      "\t Val. Loss: 0.198 |  Val. Acc: 93.32% |  Val. F1 Mac: 87.02% |  Val. F1 Mic: 93.34%\n",
      "\n",
      "Epoch: 12 | Epoch Time: 0m 14s\n",
      "Learning Rate: 3.5e-05\n",
      "\tTrain Loss: 0.119 | Train Acc: 95.89% | Train F1 Mac: 91.67% | Train F1 Mic: 95.92%\n",
      "\t Val. Loss: 0.207 |  Val. Acc: 93.11% |  Val. F1 Mac: 86.75% |  Val. F1 Mic: 93.16%\n",
      "\n",
      "Epoch: 13 | Epoch Time: 0m 14s\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "\tTrain Loss: 0.105 | Train Acc: 96.44% | Train F1 Mac: 92.72% | Train F1 Mic: 96.51%\n",
      "\t Val. Loss: 0.210 |  Val. Acc: 93.33% |  Val. F1 Mac: 87.33% |  Val. F1 Mic: 93.28%\n",
      "\n",
      "Epoch: 14 | Epoch Time: 0m 17s\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "\tTrain Loss: 0.093 | Train Acc: 97.01% | Train F1 Mac: 93.58% | Train F1 Mic: 96.97%\n",
      "\t Val. Loss: 0.210 |  Val. Acc: 93.53% |  Val. F1 Mac: 87.65% |  Val. F1 Mic: 93.49%\n",
      "\n",
      "Epoch: 15 | Epoch Time: 0m 18s\n",
      "Learning Rate: 1.2004999999999998e-05\n",
      "\tTrain Loss: 0.088 | Train Acc: 97.11% | Train F1 Mac: 94.12% | Train F1 Mic: 97.20%\n",
      "\t Val. Loss: 0.217 |  Val. Acc: 93.47% |  Val. F1 Mac: 87.57% |  Val. F1 Mic: 93.48%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1_mac, train_f1_mic = train(model, train_loader, optimizer, criterion, 0)\n",
    "    valid_loss, valid_acc, valid_f1_mac, valid_f1_mic = evaluate(model, eval_loader, criterion, 0)\n",
    "    \n",
    "    cur_lr = scheduler.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'event_tagger.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'Learning Rate: {cur_lr}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%', end=' | ')\n",
    "    print(f'Train F1 Mac: {train_f1_mac*100:.2f}% | Train F1 Mic: {train_f1_mic*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%', end=' |  ')\n",
    "    print(f'Val. F1 Mac: {valid_f1_mac*100:.2f}% |  Val. F1 Mic: {valid_f1_mic*100:.2f}%', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

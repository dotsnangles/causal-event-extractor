{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator')\n",
    "tokenizer = ElectraTokenizer.from_pretrained('tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['E_B', 'E_I', 'O', '[PAD]']\n",
    "num_labels = len(labels)\n",
    "id2label = {k: v for k, v in enumerate(labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Preprocess for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda x: ['[CLS]'] + x + ['[SEP]'])\n",
    "data['labels'] = data.labels.apply(lambda x: ['O'] + x + ['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tokens.apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda x: x + ['[PAD]'] * (max_len - len(x)))\n",
    "data['labels'] = data.labels.apply(lambda x: x + ['[PAD]'] * (max_len - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lst = data.tokens.to_list()\n",
    "labels_lst = data.labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(tokens_lst, \n",
    "                                                    labels_lst, \n",
    "                                                    test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for tokens, labels in zip(X_train, y_train):\n",
    "    length = tokens.index('[PAD]')\n",
    "    mask = [1] * length + [0] * (max_len - length)\n",
    "\n",
    "    label_ids = []\n",
    "    for label in labels:\n",
    "        label_ids.append(label2id[label])\n",
    "        \n",
    "    train_data.append([tokenizer.convert_tokens_to_ids(tokens), mask, label_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "for tokens, labels in zip(X_eval, y_eval):\n",
    "    length = tokens.index('[PAD]')\n",
    "    mask = [1] * length + [0] * (max_len - length)\n",
    "    \n",
    "    label_ids = []\n",
    "    for label in labels:\n",
    "        label_ids.append(label2id[label])\n",
    "        \n",
    "    eval_data.append([tokenizer.convert_tokens_to_ids(tokens), mask, label_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = random.randrange(0, len(train_data) - 1)\n",
    "# for x, xm, y in zip(train_data[idx][0], train_data[idx][1], train_data[idx][2]):\n",
    "#     print(x, xm, y)\n",
    "\n",
    "# idx = random.randrange(0, len(eval_data) - 1)\n",
    "# for x, xm, y in zip(eval_data[idx][0], eval_data[idx][1], eval_data[idx][2]):\n",
    "#     print(x, xm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggerDataset(Dataset): \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx][0]\n",
    "        mask = self.data[idx][1]\n",
    "        label_ids = self.data[idx][2]\n",
    "        return (torch.LongTensor(input_ids), torch.LongTensor(mask), torch.LongTensor(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggerDataset(train_data)\n",
    "eval_dataset = TaggerDataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 128, shuffle = True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = 128, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTPoSTagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 output_dim, \n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        self.fc = nn.Linear(embedding_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, mask):\n",
    "        embedded = self.dropout(self.bert(text, mask)[0])\n",
    "        predictions = self.fc(self.dropout(embedded))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(36223, 768)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIM = num_labels\n",
    "DROPOUT = 0.25\n",
    "\n",
    "model = BERTPoSTagger(bert,\n",
    "                      OUTPUT_DIM, \n",
    "                      DROPOUT)\n",
    "model.bert.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 4])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, mask, label_ids = train_dataset[0]\n",
    "input_ids, mask = input_ids.unsqueeze(0), mask.unsqueeze(0)\n",
    "\n",
    "output = model(input_ids, mask)\n",
    "output.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeonghyeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

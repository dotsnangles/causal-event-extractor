{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 10:18:01.308135: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-20 10:18:02.801628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchcrf import CRF\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['[PAD]', 'E_B', 'E_I', 'O']\n",
    "num_labels = len(labels)\n",
    "id2label = {k: v for k, v in enumerate(labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=4)\n",
    "tokenizer = ElectraTokenizer.from_pretrained('tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Preprocess for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda x: ['[CLS]'] + x + ['[SEP]'])\n",
    "data['labels'] = data.labels.apply(lambda x: ['O'] + x + ['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tokens.apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda x: x + ['[PAD]'] * (max_len - len(x)))\n",
    "data['labels'] = data.labels.apply(lambda x: x + ['[PAD]'] * (max_len - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lst = data.tokens.to_list()\n",
    "labels_lst = data.labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(tokens_lst, \n",
    "                                                    labels_lst, \n",
    "                                                    test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for tokens, labels in zip(X_train, y_train):\n",
    "    length = tokens.index('[PAD]')\n",
    "    mask = [1] * length + [0] * (max_len - length)\n",
    "\n",
    "    label_ids = []\n",
    "    for label in labels:\n",
    "        label_ids.append(label2id[label])\n",
    "        \n",
    "    train_data.append([tokenizer.convert_tokens_to_ids(tokens), mask, label_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "for tokens, labels in zip(X_eval, y_eval):\n",
    "    length = tokens.index('[PAD]')\n",
    "    mask = [1] * length + [0] * (max_len - length)\n",
    "    \n",
    "    label_ids = []\n",
    "    for label in labels:\n",
    "        label_ids.append(label2id[label])\n",
    "        \n",
    "    eval_data.append([tokenizer.convert_tokens_to_ids(tokens), mask, label_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = random.randrange(0, len(train_data) - 1)\n",
    "# for x, xm, y in zip(train_data[idx][0], train_data[idx][1], train_data[idx][2]):\n",
    "#     print(x, xm, y)\n",
    "\n",
    "# idx = random.randrange(0, len(eval_data) - 1)\n",
    "# for x, xm, y in zip(eval_data[idx][0], eval_data[idx][1], eval_data[idx][2]):\n",
    "#     print(x, xm, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and Generate Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggerDataset(Dataset): \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx][0]\n",
    "        mask = self.data[idx][1]\n",
    "        label_ids = self.data[idx][2]\n",
    "        return (torch.LongTensor(input_ids), torch.LongTensor(mask), torch.LongTensor(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggerDataset(train_data)\n",
    "eval_dataset = TaggerDataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert, config, need_birnn=False, rnn_dim=128):\n",
    "        super(BERT_BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        self.num_tags = config.num_labels\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        out_dim = config.hidden_size\n",
    "        self.need_birnn = need_birnn\n",
    "\n",
    "        # if False, no use of BiLSTM\n",
    "        if need_birnn:\n",
    "            self.birnn = nn.LSTM(config.hidden_size, rnn_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "            out_dim = rnn_dim*2\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(out_dim, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "    \n",
    "    def predict(self, input_ids, input_mask=None):\n",
    "        emissions = self.tag_outputs(input_ids, input_mask)\n",
    "        return self.crf.decode(emissions, input_mask.byte())\n",
    "\n",
    "    def forward(self, input_ids, tags, input_mask=None):\n",
    "        emissions = self.tag_outputs(input_ids, input_mask)\n",
    "        loss = -1*self.crf(emissions, tags.long(), input_mask.byte()) # negative log likelihood loss , reduction='mean' default 'sum'\n",
    "        return loss.unsqueeze(0)\n",
    "\n",
    "    def tag_outputs(self, input_ids, input_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=input_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        if self.need_birnn:\n",
    "            self.birnn.flatten_parameters()\n",
    "            sequence_output, _ = self.birnn(sequence_output)\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.hidden2tag(sequence_output)\n",
    "        return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.1\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "bert.resize_token_embeddings(len(tokenizer))\n",
    "config = bert.config\n",
    "print(config.num_labels)\n",
    "print(config.hidden_dropout_prob)\n",
    "print(config.hidden_size)\n",
    "\n",
    "model = BERT_BiLSTM_CRF(bert, config, need_birnn=True, rnn_dim=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer / Criterion / Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=0, factor=0.7, min_lr=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NGPU = torch.cuda.device_count()\n",
    "if NGPU > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(NGPU)))\n",
    "    # model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
    "    # torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    non_pad_elements = torch.nonzero((y != tag_pad_idx))\n",
    "    correct = preds[non_pad_elements].eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_f1(preds, y, tag_pad_idx):\n",
    "    non_pad_elements = torch.nonzero((y != tag_pad_idx))\n",
    "    preds_no_pad = preds[non_pad_elements].squeeze(1).detach().cpu()\n",
    "    y_no_pad = y[non_pad_elements].detach().cpu()\n",
    "    \n",
    "    f1_macro = f1_score(y_no_pad, preds_no_pad, average='macro')\n",
    "    f1_micro = f1_score(y_no_pad, preds_no_pad, average='micro')    \n",
    "    \n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, tag_pad_idx):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    predictions_set = None\n",
    "    tags_set = None\n",
    "    for batch in iterator:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        tags = batch[2].to(device)\n",
    "\n",
    "        loss = model(input_ids, tags, attention_mask).mean() / batch_size\n",
    "        \n",
    "        predictions = model.module.predict(input_ids, attention_mask)\n",
    "        predictions = list(map(lambda x: x + [0 for _ in range(max_len - len(x))], predictions))\n",
    "        predictions = torch.LongTensor(predictions).to(device)\n",
    "        predictions = predictions.view(-1)\n",
    "        tags = tags.view(-1)\n",
    "        if predictions_set == None:\n",
    "            predictions_set = predictions\n",
    "            tags_set = tags\n",
    "        else:\n",
    "            predictions_set = torch.cat([predictions_set, predictions], dim=0)\n",
    "            tags_set = torch.cat([tags_set, tags], dim=0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    f1_macro, f1_micro = categorical_f1(predictions_set, tags_set, tag_pad_idx)\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, tag_pad_idx):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    predictions_set = None\n",
    "    tags_set = None\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            tags = batch[2].to(device)\n",
    "            \n",
    "            loss = model(input_ids, tags, attention_mask).mean() / batch_size\n",
    "            \n",
    "            predictions = model.module.predict(input_ids, attention_mask)\n",
    "            predictions = list(map(lambda x: x + [0 for _ in range(max_len - len(x))], predictions))\n",
    "            predictions = torch.LongTensor(predictions).to(device)\n",
    "            predictions = predictions.view(-1)\n",
    "            tags = tags.view(-1)\n",
    "            if predictions_set == None:\n",
    "                predictions_set = predictions\n",
    "                tags_set = tags\n",
    "            else:\n",
    "                predictions_set = torch.cat([predictions_set, predictions], dim=0)\n",
    "                tags_set = torch.cat([tags_set, tags], dim=0)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        f1_macro, f1_micro = categorical_f1(predictions_set, tags_set, tag_pad_idx)\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/envs/pytorch/lib/python3.11/site-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /home/conda/feedstock_root/build_artifacts/pytorch-recipe_1673745441827/work/aten/src/ATen/native/TensorCompare.cpp:413.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 5m 12s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 6.470 | Train Acc: 80.35% | Train F1 Mac: 38.09% | Train F1 Mic: 80.47%\n",
      "\t Val. Loss: 3.103 |  Val. Acc: 90.97% |  Val. F1 Mac: 82.79% |  Val. F1 Mic: 90.96%\n",
      "\n",
      "Epoch: 02 | Epoch Time: 4m 13s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 2.416 | Train Acc: 92.88% | Train F1 Mac: 86.46% | Train F1 Mic: 92.84%\n",
      "\t Val. Loss: 2.346 |  Val. Acc: 92.98% |  Val. F1 Mac: 86.91% |  Val. F1 Mic: 92.98%\n",
      "\n",
      "Epoch: 03 | Epoch Time: 4m 21s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 1.571 | Train Acc: 95.43% | Train F1 Mac: 91.15% | Train F1 Mic: 95.43%\n",
      "\t Val. Loss: 2.384 |  Val. Acc: 92.96% |  Val. F1 Mac: 87.16% |  Val. F1 Mic: 92.92%\n",
      "\n",
      "Epoch: 04 | Epoch Time: 4m 25s\n",
      "Learning Rate: 3.5e-05\n",
      "\tTrain Loss: 1.043 | Train Acc: 97.05% | Train F1 Mac: 94.15% | Train F1 Mic: 97.05%\n",
      "\t Val. Loss: 2.251 |  Val. Acc: 93.81% |  Val. F1 Mac: 87.84% |  Val. F1 Mic: 93.65%\n",
      "\n",
      "Epoch: 05 | Epoch Time: 4m 20s\n",
      "Learning Rate: 3.5e-05\n",
      "\tTrain Loss: 0.797 | Train Acc: 97.91% | Train F1 Mac: 95.76% | Train F1 Mic: 97.91%\n",
      "\t Val. Loss: 2.437 |  Val. Acc: 93.51% |  Val. F1 Mac: 87.54% |  Val. F1 Mic: 93.55%\n",
      "\n",
      "Epoch: 06 | Epoch Time: 4m 14s\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "\tTrain Loss: 0.594 | Train Acc: 98.40% | Train F1 Mac: 96.82% | Train F1 Mic: 98.43%\n",
      "\t Val. Loss: 2.766 |  Val. Acc: 93.54% |  Val. F1 Mac: 87.65% |  Val. F1 Mic: 93.52%\n",
      "\n",
      "Epoch: 07 | Epoch Time: 4m 23s\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "\tTrain Loss: 0.457 | Train Acc: 98.78% | Train F1 Mac: 97.54% | Train F1 Mic: 98.80%\n",
      "\t Val. Loss: 2.920 |  Val. Acc: 93.63% |  Val. F1 Mac: 87.93% |  Val. F1 Mic: 93.51%\n",
      "\n",
      "Epoch: 08 | Epoch Time: 4m 31s\n",
      "Learning Rate: 1.2004999999999998e-05\n",
      "\tTrain Loss: 0.385 | Train Acc: 98.96% | Train F1 Mac: 97.92% | Train F1 Mic: 98.97%\n",
      "\t Val. Loss: 2.996 |  Val. Acc: 93.86% |  Val. F1 Mac: 87.91% |  Val. F1 Mic: 93.65%\n",
      "\n",
      "Epoch: 09 | Epoch Time: 4m 12s\n",
      "Learning Rate: 8.403499999999998e-06\n",
      "\tTrain Loss: 0.343 | Train Acc: 99.12% | Train F1 Mac: 98.25% | Train F1 Mic: 99.14%\n",
      "\t Val. Loss: 3.142 |  Val. Acc: 93.60% |  Val. F1 Mac: 87.66% |  Val. F1 Mic: 93.39%\n",
      "\n",
      "Epoch: 10 | Epoch Time: 4m 21s\n",
      "Learning Rate: 5.882449999999998e-06\n",
      "\tTrain Loss: 0.312 | Train Acc: 99.15% | Train F1 Mac: 98.34% | Train F1 Mic: 99.17%\n",
      "\t Val. Loss: 3.260 |  Val. Acc: 93.67% |  Val. F1 Mac: 87.86% |  Val. F1 Mic: 93.69%\n",
      "\n",
      "Epoch: 11 | Epoch Time: 4m 28s\n",
      "Learning Rate: 4.117714999999998e-06\n",
      "\tTrain Loss: 0.291 | Train Acc: 99.23% | Train F1 Mac: 98.50% | Train F1 Mic: 99.25%\n",
      "\t Val. Loss: 3.263 |  Val. Acc: 93.78% |  Val. F1 Mac: 87.98% |  Val. F1 Mic: 93.65%\n",
      "\n",
      "Epoch: 12 | Epoch Time: 4m 22s\n",
      "Learning Rate: 2.8824004999999986e-06\n",
      "\tTrain Loss: 0.278 | Train Acc: 99.21% | Train F1 Mac: 98.48% | Train F1 Mic: 99.24%\n",
      "\t Val. Loss: 3.345 |  Val. Acc: 93.74% |  Val. F1 Mac: 87.90% |  Val. F1 Mic: 93.63%\n",
      "\n",
      "Epoch: 13 | Epoch Time: 4m 11s\n",
      "Learning Rate: 2.017680349999999e-06\n",
      "\tTrain Loss: 0.277 | Train Acc: 99.24% | Train F1 Mac: 98.50% | Train F1 Mic: 99.26%\n",
      "\t Val. Loss: 3.335 |  Val. Acc: 93.60% |  Val. F1 Mac: 87.88% |  Val. F1 Mic: 93.61%\n",
      "\n",
      "Epoch: 14 | Epoch Time: 4m 38s\n",
      "Learning Rate: 1.4123762449999993e-06\n",
      "\tTrain Loss: 0.274 | Train Acc: 99.28% | Train F1 Mac: 98.58% | Train F1 Mic: 99.29%\n",
      "\t Val. Loss: 3.363 |  Val. Acc: 93.67% |  Val. F1 Mac: 87.93% |  Val. F1 Mic: 93.65%\n",
      "\n",
      "Epoch: 15 | Epoch Time: 4m 23s\n",
      "Learning Rate: 9.886633714999995e-07\n",
      "\tTrain Loss: 0.262 | Train Acc: 99.27% | Train F1 Mac: 98.54% | Train F1 Mic: 99.27%\n",
      "\t Val. Loss: 3.391 |  Val. Acc: 93.81% |  Val. F1 Mac: 87.95% |  Val. F1 Mic: 93.65%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1_mac, train_f1_mic = train(model, train_loader, optimizer, 0)\n",
    "    valid_loss, valid_acc, valid_f1_mac, valid_f1_mic = evaluate(model, eval_loader, 0)\n",
    "    \n",
    "    cur_lr = scheduler.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'event_tagger.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'Learning Rate: {cur_lr}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%', end=' | ')\n",
    "    print(f'Train F1 Mac: {train_f1_mac*100:.2f}% | Train F1 Mic: {train_f1_mic*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%', end=' |  ')\n",
    "    print(f'Val. F1 Mac: {valid_f1_mac*100:.2f}% |  Val. F1 Mic: {valid_f1_mic*100:.2f}%', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'last.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeonghyeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

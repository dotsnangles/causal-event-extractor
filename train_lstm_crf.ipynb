{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchcrf import CRF\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['[PAD]', 'E_B', 'E_I', 'O']\n",
    "num_labels = len(labels)\n",
    "id2label = {k: v for k, v in enumerate(labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load BERT and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = ElectraModel.from_pretrained('monologg/koelectra-base-v3-discriminator', num_labels=4)\n",
    "tokenizer = ElectraTokenizer.from_pretrained('tokenizer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Preprocess for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('data/preprocessed.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda x: ['[CLS]'] + x + ['[SEP]'])\n",
    "data['labels'] = data.labels.apply(lambda x: ['O'] + x + ['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tokens.apply(lambda x: len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tokens'] = data.tokens.apply(lambda x: x + ['[PAD]'] * (max_len - len(x)))\n",
    "data['labels'] = data.labels.apply(lambda x: x + ['[PAD]'] * (max_len - len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_lst = data.tokens.to_list()\n",
    "labels_lst = data.labels.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(tokens_lst, \n",
    "                                                    labels_lst, \n",
    "                                                    test_size=0.2, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for tokens, labels in zip(X_train, y_train):\n",
    "    length = tokens.index('[PAD]')\n",
    "    mask = [1] * length + [0] * (max_len - length)\n",
    "\n",
    "    label_ids = []\n",
    "    for label in labels:\n",
    "        label_ids.append(label2id[label])\n",
    "        \n",
    "    train_data.append([tokenizer.convert_tokens_to_ids(tokens), mask, label_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = []\n",
    "for tokens, labels in zip(X_eval, y_eval):\n",
    "    length = tokens.index('[PAD]')\n",
    "    mask = [1] * length + [0] * (max_len - length)\n",
    "    \n",
    "    label_ids = []\n",
    "    for label in labels:\n",
    "        label_ids.append(label2id[label])\n",
    "        \n",
    "    eval_data.append([tokenizer.convert_tokens_to_ids(tokens), mask, label_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = random.randrange(0, len(train_data) - 1)\n",
    "# for x, xm, y in zip(train_data[idx][0], train_data[idx][1], train_data[idx][2]):\n",
    "#     print(x, xm, y)\n",
    "\n",
    "# idx = random.randrange(0, len(eval_data) - 1)\n",
    "# for x, xm, y in zip(eval_data[idx][0], eval_data[idx][1], eval_data[idx][2]):\n",
    "#     print(x, xm, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HP Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "LEARNING_RATE = 5e-5\n",
    "N_EPOCHS = 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset and Generate Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaggerDataset(Dataset): \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = self.data[idx][0]\n",
    "        mask = self.data[idx][1]\n",
    "        label_ids = self.data[idx][2]\n",
    "        return (torch.LongTensor(input_ids), torch.LongTensor(mask), torch.LongTensor(label_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TaggerDataset(train_data)\n",
    "eval_dataset = TaggerDataset(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_BiLSTM_CRF(nn.Module):\n",
    "    \n",
    "    def __init__(self, bert, config, need_birnn=False, rnn_dim=128):\n",
    "        super(BERT_BiLSTM_CRF, self).__init__()\n",
    "        \n",
    "        self.num_tags = config.num_labels\n",
    "        self.bert = bert\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        out_dim = config.hidden_size\n",
    "        self.need_birnn = need_birnn\n",
    "\n",
    "        # if False, no use of BiLSTM\n",
    "        if need_birnn:\n",
    "            self.birnn = nn.LSTM(config.hidden_size, rnn_dim, num_layers=1, bidirectional=True, batch_first=True)\n",
    "            out_dim = rnn_dim*2\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(out_dim, config.num_labels)\n",
    "        self.crf = CRF(config.num_labels, batch_first=True)\n",
    "    \n",
    "    def predict(self, input_ids, input_mask=None):\n",
    "        emissions = self.tag_outputs(input_ids, input_mask)\n",
    "        return self.crf.decode(emissions, input_mask.byte())\n",
    "\n",
    "    def forward(self, input_ids, tags, input_mask=None):\n",
    "        emissions = self.tag_outputs(input_ids, input_mask)\n",
    "        loss = -1*self.crf(emissions, tags.long(), input_mask.byte()) # negative log likelihood loss\n",
    "        return loss.unsqueeze(0)\n",
    "\n",
    "    def tag_outputs(self, input_ids, input_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=input_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        \n",
    "        if self.need_birnn:\n",
    "            self.birnn.flatten_parameters()\n",
    "            sequence_output, _ = self.birnn(sequence_output)\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        emissions = self.hidden2tag(sequence_output)\n",
    "        return emissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.1\n",
      "768\n"
     ]
    }
   ],
   "source": [
    "bert.resize_token_embeddings(len(tokenizer))\n",
    "config = bert.config\n",
    "print(config.num_labels)\n",
    "print(config.hidden_dropout_prob)\n",
    "print(config.hidden_size)\n",
    "\n",
    "model = BERT_BiLSTM_CRF(bert, config, need_birnn=True, rnn_dim=128)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer / Criterion / Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr = LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=0, factor=0.7, min_lr=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "NGPU = torch.cuda.device_count()\n",
    "if NGPU > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(NGPU)))\n",
    "    # model = torch.nn.DataParallel(model, device_ids=[0,1])\n",
    "    # torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_accuracy(preds, y, tag_pad_idx):\n",
    "    non_pad_elements = torch.nonzero((y != tag_pad_idx))\n",
    "    correct = preds[non_pad_elements].eq(y[non_pad_elements])\n",
    "    return correct.sum() / torch.FloatTensor([y[non_pad_elements].shape[0]]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_f1(preds, y, tag_pad_idx):\n",
    "    non_pad_elements = torch.nonzero((y != tag_pad_idx))\n",
    "    preds_no_pad = preds[non_pad_elements].squeeze(1).detach().cpu()\n",
    "    y_no_pad = y[non_pad_elements].detach().cpu()\n",
    "    \n",
    "    f1_macro = f1_score(y_no_pad, preds_no_pad, average='macro')\n",
    "    f1_micro = f1_score(y_no_pad, preds_no_pad, average='micro')    \n",
    "    \n",
    "    return f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, tag_pad_idx):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    predictions_set = None\n",
    "    tags_set = None\n",
    "    for batch in iterator:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        tags = batch[2].to(device)\n",
    "\n",
    "        loss = model(input_ids, tags, attention_mask).mean() / batch_size\n",
    "        \n",
    "        predictions = model.module.predict(input_ids, attention_mask)\n",
    "        predictions = list(map(lambda x: x + [0 for _ in range(max_len - len(x))], predictions))\n",
    "        predictions = torch.LongTensor(predictions).to(device)\n",
    "        predictions = predictions.view(-1)\n",
    "        tags = tags.view(-1)\n",
    "        if predictions_set == None:\n",
    "            predictions_set = predictions\n",
    "            tags_set = tags\n",
    "        else:\n",
    "            predictions_set = torch.cat([predictions_set, predictions], dim=0)\n",
    "            tags_set = torch.cat([tags_set, tags], dim=0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    f1_macro, f1_micro = categorical_f1(predictions_set, tags_set, tag_pad_idx)\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, tag_pad_idx):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    predictions_set = None\n",
    "    tags_set = None\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch[0].to(device)\n",
    "            attention_mask = batch[1].to(device)\n",
    "            tags = batch[2].to(device)\n",
    "            \n",
    "            loss = model(input_ids, tags, attention_mask).mean() / batch_size\n",
    "            \n",
    "            predictions = model.module.predict(input_ids, attention_mask)\n",
    "            predictions = list(map(lambda x: x + [0 for _ in range(max_len - len(x))], predictions))\n",
    "            predictions = torch.LongTensor(predictions).to(device)\n",
    "            predictions = predictions.view(-1)\n",
    "            tags = tags.view(-1)\n",
    "            if predictions_set == None:\n",
    "                predictions_set = predictions\n",
    "                tags_set = tags\n",
    "            else:\n",
    "                predictions_set = torch.cat([predictions_set, predictions], dim=0)\n",
    "                tags_set = torch.cat([tags_set, tags], dim=0)\n",
    "            \n",
    "            acc = categorical_accuracy(predictions, tags, tag_pad_idx)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        f1_macro, f1_micro = categorical_f1(predictions_set, tags_set, tag_pad_idx)\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator), f1_macro, f1_micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 3m 20s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 7.079 | Train Acc: 79.05% | Train F1 Mac: 33.28% | Train F1 Mic: 79.09%\n",
      "\t Val. Loss: 4.125 |  Val. Acc: 87.41% |  Val. F1 Mac: 55.69% |  Val. F1 Mic: 87.34%\n",
      "\n",
      "Epoch: 02 | Epoch Time: 3m 6s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 3.366 | Train Acc: 90.25% | Train F1 Mac: 74.85% | Train F1 Mic: 90.20%\n",
      "\t Val. Loss: 2.668 |  Val. Acc: 92.75% |  Val. F1 Mac: 85.18% |  Val. F1 Mic: 92.67%\n",
      "\n",
      "Epoch: 03 | Epoch Time: 3m 6s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 2.054 | Train Acc: 94.51% | Train F1 Mac: 88.48% | Train F1 Mic: 94.52%\n",
      "\t Val. Loss: 2.406 |  Val. Acc: 93.48% |  Val. F1 Mac: 86.44% |  Val. F1 Mic: 93.39%\n",
      "\n",
      "Epoch: 04 | Epoch Time: 3m 6s\n",
      "Learning Rate: 5e-05\n",
      "\tTrain Loss: 1.480 | Train Acc: 96.00% | Train F1 Mac: 91.42% | Train F1 Mic: 95.99%\n",
      "\t Val. Loss: 2.489 |  Val. Acc: 93.58% |  Val. F1 Mac: 86.43% |  Val. F1 Mic: 93.33%\n",
      "\n",
      "Epoch: 05 | Epoch Time: 3m 6s\n",
      "Learning Rate: 3.5e-05\n",
      "\tTrain Loss: 1.074 | Train Acc: 97.32% | Train F1 Mac: 94.10% | Train F1 Mic: 97.31%\n",
      "\t Val. Loss: 2.684 |  Val. Acc: 93.61% |  Val. F1 Mac: 86.92% |  Val. F1 Mic: 93.40%\n",
      "\n",
      "Epoch: 06 | Epoch Time: 3m 6s\n",
      "Learning Rate: 2.4499999999999996e-05\n",
      "\tTrain Loss: 0.804 | Train Acc: 98.12% | Train F1 Mac: 95.79% | Train F1 Mic: 98.13%\n",
      "\t Val. Loss: 2.829 |  Val. Acc: 93.62% |  Val. F1 Mac: 87.02% |  Val. F1 Mic: 93.50%\n",
      "\n",
      "Epoch: 07 | Epoch Time: 3m 6s\n",
      "Learning Rate: 1.7149999999999997e-05\n",
      "\tTrain Loss: 0.664 | Train Acc: 98.48% | Train F1 Mac: 96.63% | Train F1 Mic: 98.50%\n",
      "\t Val. Loss: 2.944 |  Val. Acc: 92.78% |  Val. F1 Mac: 87.25% |  Val. F1 Mic: 93.58%\n",
      "\n",
      "Epoch: 08 | Epoch Time: 3m 6s\n",
      "Learning Rate: 1.2004999999999998e-05\n",
      "\tTrain Loss: 0.580 | Train Acc: 98.67% | Train F1 Mac: 97.04% | Train F1 Mic: 98.67%\n",
      "\t Val. Loss: 2.932 |  Val. Acc: 93.88% |  Val. F1 Mac: 87.29% |  Val. F1 Mic: 93.64%\n",
      "\n",
      "Epoch: 09 | Epoch Time: 3m 6s\n",
      "Learning Rate: 8.403499999999998e-06\n",
      "\tTrain Loss: 0.528 | Train Acc: 98.74% | Train F1 Mac: 97.19% | Train F1 Mic: 98.74%\n",
      "\t Val. Loss: 3.065 |  Val. Acc: 93.54% |  Val. F1 Mac: 86.93% |  Val. F1 Mic: 93.55%\n",
      "\n",
      "Epoch: 10 | Epoch Time: 3m 6s\n",
      "Learning Rate: 5.882449999999998e-06\n",
      "\tTrain Loss: 0.488 | Train Acc: 98.90% | Train F1 Mac: 97.55% | Train F1 Mic: 98.89%\n",
      "\t Val. Loss: 3.077 |  Val. Acc: 93.74% |  Val. F1 Mac: 86.99% |  Val. F1 Mic: 93.55%\n",
      "\n",
      "Epoch: 11 | Epoch Time: 3m 6s\n",
      "Learning Rate: 4.117714999999998e-06\n",
      "\tTrain Loss: 0.452 | Train Acc: 98.87% | Train F1 Mac: 97.55% | Train F1 Mic: 98.89%\n",
      "\t Val. Loss: 3.126 |  Val. Acc: 93.70% |  Val. F1 Mac: 86.98% |  Val. F1 Mic: 93.52%\n",
      "\n",
      "Epoch: 12 | Epoch Time: 3m 7s\n",
      "Learning Rate: 2.8824004999999986e-06\n",
      "\tTrain Loss: 0.450 | Train Acc: 98.95% | Train F1 Mac: 97.71% | Train F1 Mic: 98.97%\n",
      "\t Val. Loss: 3.171 |  Val. Acc: 92.97% |  Val. F1 Mac: 86.89% |  Val. F1 Mic: 93.45%\n",
      "\n",
      "Epoch: 13 | Epoch Time: 3m 7s\n",
      "Learning Rate: 2.017680349999999e-06\n",
      "\tTrain Loss: 0.425 | Train Acc: 98.96% | Train F1 Mac: 97.74% | Train F1 Mic: 98.98%\n",
      "\t Val. Loss: 3.171 |  Val. Acc: 93.68% |  Val. F1 Mac: 86.98% |  Val. F1 Mic: 93.53%\n",
      "\n",
      "Epoch: 14 | Epoch Time: 3m 5s\n",
      "Learning Rate: 1.4123762449999993e-06\n",
      "\tTrain Loss: 0.428 | Train Acc: 99.00% | Train F1 Mac: 97.80% | Train F1 Mic: 99.01%\n",
      "\t Val. Loss: 3.183 |  Val. Acc: 93.39% |  Val. F1 Mac: 86.91% |  Val. F1 Mic: 93.51%\n",
      "\n",
      "Epoch: 15 | Epoch Time: 3m 6s\n",
      "Learning Rate: 9.886633714999995e-07\n",
      "\tTrain Loss: 0.420 | Train Acc: 99.04% | Train F1 Mac: 97.90% | Train F1 Mic: 99.05%\n",
      "\t Val. Loss: 3.184 |  Val. Acc: 93.62% |  Val. F1 Mac: 86.93% |  Val. F1 Mic: 93.51%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1_mac, train_f1_mic = train(model, train_loader, optimizer, 0)\n",
    "    valid_loss, valid_acc, valid_f1_mac, valid_f1_mic = evaluate(model, eval_loader, 0)\n",
    "    \n",
    "    cur_lr = scheduler.optimizer.state_dict()['param_groups'][0]['lr']\n",
    "    scheduler.step(valid_loss)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'event_tagger.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'Learning Rate: {cur_lr}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%', end=' | ')\n",
    "    print(f'Train F1 Mac: {train_f1_mac*100:.2f}% | Train F1 Mic: {train_f1_mic*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%', end=' |  ')\n",
    "    print(f'Val. F1 Mac: {valid_f1_mac*100:.2f}% |  Val. F1 Mic: {valid_f1_mic*100:.2f}%', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'last.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jeonghyeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "df12b971f0e4e081474c4ac44bd338416eac6f5401e1e938ba342788cee78ecd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
